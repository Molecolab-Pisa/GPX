{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e106c37d-2fc6-4b24-945e-963f5b8889cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c73bd0-3ea1-4055-b0b1-232c78d9d834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "# GPs generally need double precision\n",
    "# importing gpx sets the double precision by default for jax\n",
    "# otherwise, use jax.config.update('jax_enable_x64', True)\n",
    "import gpx\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d235b0-2dbc-4e43-b3e9-ab459cc910a1",
   "metadata": {},
   "source": [
    "# Kernelizing a kernel function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6f836d-4fe2-409b-8b15-a12b25e64ac5",
   "metadata": {},
   "source": [
    "This notebooks serves to demonstrate the usefulness of the `kernelize` decorator.\n",
    "\n",
    "This idea can be found in https://github.com/google/jax/blob/main/examples/gaussian_process_regression.py, under the name of `cov_map` (but in that example the order of the arguments is inverted...).\n",
    "In addition to the above cited implementation, in `gpx` there is also another implementation that makes use of the `jax.lax` operations to obtain the same result.\n",
    "\n",
    "The `kernelize` decorator takes as input a kernel function, and gives back a kernel function operating on batches of data.\n",
    "It is very useful as the input kernel function, which we call `kernel_base`, is written taking as input a single data point, while the output of the `kernelize` function takes as input an entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077519ce-010f-4f2a-a5e3-d86b69d6fb9a",
   "metadata": {},
   "source": [
    "We write our simple `kernel_base` as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32fc65eb-4933-4807-b3ef-08a7b97e7061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_base(x1, x2, params):\n",
    "    return jnp.exp(-jnp.sum((x1 - x2) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd6ece9-60fd-4a38-8eae-0178933607e6",
   "metadata": {},
   "source": [
    "This function has the same signature of the `gpx` kernels, but does not make use of the `params` argument, just to keep it simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc81ec9-c0ab-4320-8e30-890a29ffe8f3",
   "metadata": {},
   "source": [
    "Now we can generate a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1efda2d1-1059-4930-bb0e-9b8970a07623",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "x = random.normal(random.PRNGKey(2023), shape=(100, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f6f15c-f28a-4919-87ff-fae6efa8d262",
   "metadata": {},
   "source": [
    "We can test that our kernel function works for a single data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edd9f17e-0313-4bd1-a2ff-252594b08fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.21099039, dtype=float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_base(x[0], x[1], None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fb58b8-13b9-4c1c-98e5-0948ba738d11",
   "metadata": {},
   "source": [
    "But it does not work (or at least, it does not work as intended) if we give as input the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b073c6e-796d-4916-9a39-e9ae83ad2651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1., dtype=float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_base(x, x, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5dd158-305a-405b-a8d3-422f9a5feda0",
   "metadata": {},
   "source": [
    "Which, obviously, does not make sense.\n",
    "\n",
    "We could write manually a function that works on batches of data as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2c57960-dce6-4c3a-8d43-9c62e8ea6c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_vectorized(x1, x2, params):\n",
    "    return jnp.exp(-jnp.sum((x1[:, None] - x2) ** 2, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eedb3f8c-0d58-4591-a189-ca13a3eeb3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1.00000000e+00, 2.10990388e-01, 1.17609567e-01, ...,\n",
       "        2.57938621e-02, 3.53752754e-02, 3.25855385e-02],\n",
       "       [2.10990388e-01, 1.00000000e+00, 1.11705062e-01, ...,\n",
       "        2.13500917e-03, 1.25013163e-01, 2.03252366e-01],\n",
       "       [1.17609567e-01, 1.11705062e-01, 1.00000000e+00, ...,\n",
       "        1.63086268e-01, 1.68039887e-03, 2.90493668e-02],\n",
       "       ...,\n",
       "       [2.57938621e-02, 2.13500917e-03, 1.63086268e-01, ...,\n",
       "        1.00000000e+00, 1.70217436e-04, 5.69201066e-05],\n",
       "       [3.53752754e-02, 1.25013163e-01, 1.68039887e-03, ...,\n",
       "        1.70217436e-04, 1.00000000e+00, 1.07095760e-03],\n",
       "       [3.25855385e-02, 2.03252366e-01, 2.90493668e-02, ...,\n",
       "        5.69201066e-05, 1.07095760e-03, 1.00000000e+00]], dtype=float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_vectorized(x, x, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34d2986-ae57-4b0c-bca8-cd78fbfcad14",
   "metadata": {},
   "source": [
    "Which in this case is of course very simple. Note however that this function stores a lot of distances in memory, and so it will raise an out of memory error as the dataset size increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d492c172-8229-42dc-950d-ef1621e67d0a",
   "metadata": {},
   "source": [
    "The `kernelize` decorator writes the batched version of the function for us.\n",
    "It can do that in two ways: by creating the batched version making use of the `jax.vmap` function, or by creating the batched version making use of the `jax.lax.fori_loop` function.\n",
    "\n",
    "The first one is faster for small datasets, as, like our manual version, stores in memory a lot of data.\n",
    "The second one is slower, but scales much better for bigger datasets.\n",
    "Let's try both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dcafdf8-47c8-4cbd-9404-8fa003af1ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpx.kernels.kernelizers import kernelize\n",
    "\n",
    "kernel_vmap = kernelize(kernel_base, lax=False)\n",
    "kernel_lax = kernelize(kernel_base, lax=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0803792e-c8e2-4dda-bc3e-11c3243907c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1.00000000e+00, 2.10990388e-01, 1.17609567e-01, ...,\n",
       "        2.57938621e-02, 3.53752754e-02, 3.25855385e-02],\n",
       "       [2.10990388e-01, 1.00000000e+00, 1.11705062e-01, ...,\n",
       "        2.13500917e-03, 1.25013163e-01, 2.03252366e-01],\n",
       "       [1.17609567e-01, 1.11705062e-01, 1.00000000e+00, ...,\n",
       "        1.63086268e-01, 1.68039887e-03, 2.90493668e-02],\n",
       "       ...,\n",
       "       [2.57938621e-02, 2.13500917e-03, 1.63086268e-01, ...,\n",
       "        1.00000000e+00, 1.70217436e-04, 5.69201066e-05],\n",
       "       [3.53752754e-02, 1.25013163e-01, 1.68039887e-03, ...,\n",
       "        1.70217436e-04, 1.00000000e+00, 1.07095760e-03],\n",
       "       [3.25855385e-02, 2.03252366e-01, 2.90493668e-02, ...,\n",
       "        5.69201066e-05, 1.07095760e-03, 1.00000000e+00]], dtype=float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_vmap(x, x, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48463cb9-0b53-4130-94d2-f0e518e565c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1.00000000e+00, 2.10990388e-01, 1.17609567e-01, ...,\n",
       "        2.57938621e-02, 3.53752754e-02, 3.25855385e-02],\n",
       "       [2.10990388e-01, 1.00000000e+00, 1.11705062e-01, ...,\n",
       "        2.13500917e-03, 1.25013163e-01, 2.03252366e-01],\n",
       "       [1.17609567e-01, 1.11705062e-01, 1.00000000e+00, ...,\n",
       "        1.63086268e-01, 1.68039887e-03, 2.90493668e-02],\n",
       "       ...,\n",
       "       [2.57938621e-02, 2.13500917e-03, 1.63086268e-01, ...,\n",
       "        1.00000000e+00, 1.70217436e-04, 5.69201066e-05],\n",
       "       [3.53752754e-02, 1.25013163e-01, 1.68039887e-03, ...,\n",
       "        1.70217436e-04, 1.00000000e+00, 1.07095760e-03],\n",
       "       [3.25855385e-02, 2.03252366e-01, 2.90493668e-02, ...,\n",
       "        5.69201066e-05, 1.07095760e-03, 1.00000000e+00]], dtype=float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_lax(x, x, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189eb657-789e-4f4f-9fc1-7098d42778eb",
   "metadata": {},
   "source": [
    "The output is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ab845d3-8f1d-42e1-a902-cd0808b9450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert jnp.allclose(kernel_vmap(x, x, None), kernel_lax(x, x, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631d3a0-196b-4dbd-9156-3fb2e1194469",
   "metadata": {},
   "source": [
    "To demonstrate the strengths and the weaknesses of each one, we can compute the time required by each one to compute the kernel matrix for increasing number of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea517216-739a-4062-aba8-a15e2af5915f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d087e8ab689e4cefa909c3fab816d415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vmap version failed: RESOURCE_EXHAUSTED: Out of memory allocating 20800000000 bytes.\n",
      "vmap version failed: RESOURCE_EXHAUSTED: Out of memory allocating 29952000000 bytes.\n",
      "vmap version failed: RESOURCE_EXHAUSTED: Out of memory allocating 40768000000 bytes.\n",
      "vmap version failed: RESOURCE_EXHAUSTED: Out of memory allocating 53248000000 bytes.\n",
      "vmap version failed: RESOURCE_EXHAUSTED: Out of memory allocating 67392000000 bytes.\n",
      "vmap version failed: RESOURCE_EXHAUSTED: Out of memory allocating 83200000000 bytes.\n"
     ]
    }
   ],
   "source": [
    "lax_times = []\n",
    "vmap_times = []\n",
    "\n",
    "for n in tqdm([5, 10, 100, 500, 1000, 1500, 2000, 5000, 6000, 7000, 8000, 9000, 10000]):\n",
    "    x = jax.random.normal(jax.random.PRNGKey(2023), shape=(n, 100))\n",
    "\n",
    "    try:\n",
    "        start = time.perf_counter()\n",
    "        _ = kernel_lax(x, x, None).block_until_ready()\n",
    "        lax_times.append(time.perf_counter() - start)\n",
    "    except Exception as e:\n",
    "        print(\"lax version failed:\", e)\n",
    "\n",
    "    try:\n",
    "        start = time.perf_counter()\n",
    "        _ = kernel_vmap(x, x, None).block_until_ready()\n",
    "        vmap_times.append(time.perf_counter() - start)\n",
    "    except Exception as e:\n",
    "        print(\"vmap version failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f22d43-c069-44be-98c9-8b6764b0f544",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lax_times, \"-o\", label=\"lax kernelizer\")\n",
    "plt.plot(vmap_times, \"-D\", label=\"vmap kernelizer\")\n",
    "plt.loglog()\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.xlabel(\"# samples\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd60a988-1abf-437c-aeb0-52f87b5b31aa",
   "metadata": {},
   "source": [
    "Which shows that the `vmap` version works great for a small number of samples, but as the number of samples increases, (1) it becomes slower, and (2) it eventually breaks as it cannot allocate in memory all the data it needs. The `jax.lax` implementation, on the other hand, works seamlessly for all the dataset sizes tested in this notebok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c2e9cc-9d78-437e-9c44-fbdf768790d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
